import os

from dotenv import load_dotenv
import openai
import streamlit as st
from langchain.chat_models import ChatOpenAI
from langchain.chains import RetrievalQA
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.vectorstores import FAISS

load_dotenv()

# APIã‚­ãƒ¼ã®è¨­å®š
openai.api_key = os.environ["OPENAI_API_KEY"]
#FAISS_DB_DIR = os.environ["FAISS_DB_DIR"]

st.set_page_config(page_title="TokAI 1.0",
                       page_icon="ğŸ¤–")
st.title("TokAI2.0")

# å®šæ•°å®šç¾©
USER_NAME = "user"
ASSISTANT_NAME = "assistant"

user_msg = st.chat_input("ã“ã“ã«ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å…¥åŠ›")

model = ChatOpenAI(model="gpt-3.5-turbo-16k-0613", temperature=0.9, client=openai.ChatCompletion)
faiss_db = FAISS.load_local("faiss_index/", embeddings=OpenAIEmbeddings(client=openai.ChatCompletion))

# LLMã«ã‚ˆã‚‹å›ç­”ã®ç”Ÿæˆ
qa = RetrievalQA.from_chain_type(llm=model, chain_type="stuff", retriever=faiss_db.as_retriever())
query = f"ã‚ãªãŸã¯æ±æµ·å¤§å­¦ã«ã¤ã„ã¦ã®è³ªå•ã«ç­”ãˆã‚‹ChatBotã§ã™ã€‚æ¬¡ã®è³ªå•ã«ç­”ãˆã¦ãã ã•ã„ã€‚:{user_msg}"

# ãƒãƒ£ãƒƒãƒˆãƒ­ã‚°ã‚’ä¿å­˜ã—ãŸã‚»ãƒƒã‚·ãƒ§ãƒ³æƒ…å ±ã‚’åˆæœŸåŒ–
if "chat_log" not in st.session_state:
    st.session_state.chat_log = []

if user_msg:
    # ä»¥å‰ã®ãƒãƒ£ãƒƒãƒˆãƒ­ã‚°ã‚’è¡¨ç¤º
    for chat in st.session_state.chat_log:
        with st.chat_message(chat["name"]):
            st.write(chat["msg"])

    # æœ€æ–°ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤º
    with st.chat_message(USER_NAME):
        st.write(user_msg)
  
    # ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤º
    response = qa.run(query)
    with st.chat_message(ASSISTANT_NAME):
        st.write(response)

    # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã«ãƒãƒ£ãƒƒãƒˆãƒ­ã‚°ã‚’è¿½åŠ 
    st.session_state.chat_log.append({"name": USER_NAME, "msg": user_msg})
    st.session_state.chat_log.append({"name": ASSISTANT_NAME, "msg": response})
